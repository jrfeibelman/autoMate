{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40db228d-745a-47b1-803e-5555db6bedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from enum import Enum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4a19cd1-0bf4-4666-9058-78f4f2d7e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModel(Enum):\n",
    "    Zephyr = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    Yi = \"01-ai/Yi-34B\"\n",
    "    Mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "    FalconChat = \"TheBloke/Falcon-180B-Chat-GGUF\"\n",
    "    FalconRaw = \"tiiuae/falcon-180B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efacead-0b5f-42df-8ed0-90aa49bbb4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3913b63cb84970be5dee34a9aabb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddcf6a66fd5457f8f3530aeb7d17644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d42fa8a18a44f6cb5fd8447f254735e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdefa3d29b34473baa2c4a2ef67e927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975211ab745d41a18f8c630e9e0fe6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba71835effd848f39c3a48ddc0743be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95feae806c8847c782bf94df203e1121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ce22b89ff048dbb6171cb099ce85f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca57cab68ae45d48c0d8be5715c0c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4b42c3ccfd41d4869f4eabafbade0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00081.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(HuggingFaceModel.FalconChat.value, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(HuggingFaceModel.FalconRaw.value, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(HuggingFaceModel.FalconRaw.value)\n",
    "inputs = tokenizer(\"I'm bored and looking for some fun things to do in new york city with beautiful scenery. Any suggestions?\", return_tensors=\"pt\").to(\"mps\")\n",
    "max_length = 256\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "output = output[0].to(\"cpu\")\n",
    "print(tokenizer.decode(output))\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     inputs.input_ids,\n",
    "#     max_length=max_length,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     do_sample=True,\n",
    "#     repetition_penalty=1.3,\n",
    "#     no_repeat_ngram_size=5,\n",
    "#     temperature=0.7,\n",
    "#     top_k=40,\n",
    "#     top_p=0.8,\n",
    "# )\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e28cd1-487f-498c-ba85-ae98e55ac524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147643f-e35e-4063-85e4-a561182976bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c4dbe-f7d8-4b64-8d9d-59b9c27a8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mistral_model():\n",
    "    return pipeline(\"text-generation\", model=HuggingFaceModel.Mistral.value, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "\n",
    "def get_zephyr_model():\n",
    "    return pipeline(\"text-generation\", model=HuggingFaceModel.Zephyr.value, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "\n",
    "\n",
    "def run_llm(pipe, context, msg, split=''):\n",
    "    # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": context,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": msg},\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    response = outputs[0][\"generated_text\"]\n",
    "    if len(split) > 0:\n",
    "        return response.split(split)[1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5a82d-30f1-45ff-ab8b-9b811be85555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zephyr = get_zephyr_model()\n",
    "mistral = get_mistral_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be0a56-0fc1-4413-b9b2-07ad6c417d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "context = \"You are a friendly, helpful chatbot who always responds in the style of a personal assistant to help your user be more productive\"\n",
    "msg = \"I'm bored and looking for some fun things to do in new york city with beautiful scenery. Any suggestions?\"\n",
    "split = 'assistant|>\\n'\n",
    "# response = run_llm(model, context, msg)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ca711-5a60-4e8f-82ee-fe40b344f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_llm(zephyr, context, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d303be-6f2a-4443-9a81-97961f0d8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_llm(mistral, context, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a0f4b-051d-4053-a571-6f4d1c1a4e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7a6e4-a53b-4c0a-ba3a-325b27a878d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270df91-7eac-4c1d-ac0f-46bb88a04731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yi_llm():\n",
    "    # CODE DOESN'T WORK ON MPS - Yi might require cuda\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"01-ai/Yi-34B\", device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"01-ai/Yi-34B\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "    inputs = tokenizer(\"There's a place where time stands still. A place of breath taking wonder, but also\", return_tensors=\"pt\")\n",
    "    max_length = 256\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids.to(\"mps\"),\n",
    "        max_length=max_length,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=5,\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "        top_p=0.8,\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fccb9-3773-4bba-a631-53f15394fce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
